{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from logger import utils\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "from logger.utils import traverse_dir\n",
    "\n",
    "# Cuda setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# configure loading\n",
    "args = utils.load_config('./configs/sins.yaml')\n",
    "\n",
    "# set path\n",
    "MP4_DATA_PATH   = 'preprocess/mp4'\n",
    "ORIGINAL_PATH   = 'preprocess/original/'\n",
    "DEMUCS_PATH     = 'preprocess/demucs/'\n",
    "NORM_PATH       = 'preprocess/norm/'\n",
    "TEMP_LOG_PATH   = 'temp_ffmpeg_log.txt'  # ffmpeg의 무음 감지 로그의 임시 저장 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터 전처리\n",
    "***\n",
    "1. 난 전처리가 필요없다. 배경음 제거가 완벽하고, 모든 데이터들도 특정 길이로 잘 잘려져있다.\n",
    "    - 데이터를 전부 data/train/audio/안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        data/train/audio/aaa.wav\n",
    "        data/train/audio/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.6 validation 분리단계로 점프\n",
    "***\n",
    "2. 난 거의 다 되어 있지만 데이터가 너무 길다. 특정 길이로 자르고 싶다.\n",
    "    - 데이터를 전부 preprocess/split 안에 다 집어 넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/norm/aaa.wav\n",
    "        preprocess/norm/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.4 split 단계로 점프\n",
    "***\n",
    "3. 난 배경음도 제거해야되고 데이터도 길다. 거의 날 것의 상태다.\n",
    "    - 데이터를 전부 preprocess/original 안에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/original/aaa.wav\n",
    "        preprocess/original/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1.2부터 demucs 단계로 점프\n",
    "***\n",
    "4. 난 아무것도 안되어 있고, 심지어 mp4파일이다.\n",
    "    - 데이터들 전부 preprocess/mp4에 다 집어넣고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/mp4/aaa.mp4\n",
    "        preprocess/mp4/bbb.mp4\n",
    "        ...\n",
    "        ```\n",
    "    - 1.1부터 차례대로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1데이터가 mp4인 경우 (wav만 있는 경우에는 패스)\n",
    "preprocess/mp4 안에 있는 mp4파일을 wav로 변경 해서 preprocess/original 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mp4_to_wav(input_dir : str, input_file: str, output_dir: str):\n",
    "#     \"\"\"mp4파일을 wav형식으로 변환합니다.\n",
    "#     Args:\n",
    "#         input_dir (str) : 입력 mp4파일의 path\n",
    "#         input_file (str) : 입력 mp4파일의 이름\n",
    "#         output_dir (str) : 출력 wav파일의 path\n",
    "#     \"\"\"\n",
    "#     ext = os.path.splitext(input_file)[1][1:]\n",
    "\n",
    "#     if ext != \"mp4\":\n",
    "#         return \n",
    "#     else :\n",
    "#         track = AudioSegment.from_file(os.path.join(input_dir,input_file),  format= 'mp4')\n",
    "#         track = track.set_frame_rate(44100)\n",
    "#         track.export(os.path.join(output_dir, input_file[:-4]+\".wav\"), format='wav')\n",
    "\n",
    "\n",
    "# filelist =  traverse_dir(\n",
    "#     MP4_DATA_PATH,\n",
    "#     extension='mp4',\n",
    "#     is_pure=True,\n",
    "#     is_sort=True,\n",
    "#     is_ext=True)\n",
    "\n",
    "# for file in tqdm(filelist):\n",
    "#     mp4_to_wav(MP4_DATA_PATH, file, ORIGINAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 무음제거 (Demucs)\n",
    "preprocess/original에 있는 wav파일들의 음악소리를 제거하고 목소리만 추출해서 preprocess/demucs 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sep_wav import demucs\n",
    "\n",
    "# demucs(ORIGINAL_PATH, DEMUCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 normalize\n",
    "preprocess/demucs에 있는 배경음이 제거된 데이터들을 노멀라이즈 (sample rate값을 바꿀 수 있음) 해서 preprocess/norm에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sep_wav import audio_norm\n",
    "\n",
    "# for filepath in tqdm(glob(DEMUCS_PATH+\"*.wav\"), desc=\"노멀라이징 작업 중...\"):\n",
    "#     filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "#     out_filepath = os.path.join(NORM_PATH, filename) + \".wav\"\n",
    "#     audio_norm(filepath, out_filepath, sample_rate = 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 split\n",
    "preprocess/norm에 있는 노말라이즈된 데이터들을 15초 길이로 잘라서 data/train/audio에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# for filepath in tqdm(glob(NORM_PATH+\"*.wav\"), desc=\"음원 자르는 중...\"):\n",
    "#     duration = librosa.get_duration(filename=filepath)\n",
    "#     max_last_seg_duration = 0\n",
    "#     sep_duration_final = 15\n",
    "#     sep_duration = 15\n",
    "\n",
    "#     while sep_duration > 4:\n",
    "#         last_seg_duration = duration % sep_duration\n",
    "#         if max_last_seg_duration < last_seg_duration:\n",
    "#             max_last_seg_duration = last_seg_duration\n",
    "#             sep_duration_final = sep_duration\n",
    "#         sep_duration -= 1\n",
    "\n",
    "#     filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "#     out_filepath = os.path.join(args.data.train_path,\"audio\", f\"{filename}-%04d.wav\")\n",
    "#     subprocess.run(f'ffmpeg -i \"{filepath}\" -f segment -segment_time {sep_duration_final} \"{out_filepath}\" -y', capture_output=True, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 무음 제거\n",
    "data/train/audio에 있는 잘라진 음원들 중에 무음인 파일들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sep_wav import get_ffmpeg_args\n",
    "# import subprocess\n",
    "\n",
    "# for filepath in tqdm(glob(args.data.train_path+\"/audio/*.wav\"), desc=\"무음 제거 중...\"):\n",
    "#     if os.path.exists(TEMP_LOG_PATH):\n",
    "#         os.remove(TEMP_LOG_PATH)\n",
    "\n",
    "#     ffmpeg_arg = get_ffmpeg_args(filepath)\n",
    "#     subprocess.run(ffmpeg_arg, capture_output=True, shell=True)\n",
    "\n",
    "#     start = None\n",
    "#     end = None\n",
    "\n",
    "#     with open(TEMP_LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f.readlines():\n",
    "#             line = line.strip()\n",
    "#             if \"lavfi.silence_start\" in line:\n",
    "#                 start = float(line.split(\"=\")[1])\n",
    "#             if \"lavfi.silence_end\" in line:\n",
    "#                 end = float(line.split(\"=\")[1])\n",
    "\n",
    "#     if start != None:\n",
    "#         if start == 0 and end == None:\n",
    "#             os.remove(filepath)\n",
    "            \n",
    "# if os.path.exists(TEMP_LOG_PATH):\n",
    "#         os.remove(TEMP_LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 학습 데이터 중, 일부를 validaion용으로 자동으로 보내준다.\n",
    "- data/train/audio에 있는 데이터 중 일정 비율만큼 알아서 data/val/audio로 이동시켜준다\n",
    "    - 계산식은 다음과 같다 `max(2, min(10, 전체 데이터 * 0.01))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from draw import main\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 (학습에 쓰기 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.hubert because of the following error (look up to see its traceback):\nNo module named 'tokenizers.tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1535\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\models\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     albert,\n\u001b[0;32m     17\u001b[0m     align,\n\u001b[0;32m     18\u001b[0m     altclip,\n\u001b[0;32m     19\u001b[0m     audio_spectrogram_transformer,\n\u001b[0;32m     20\u001b[0m     auto,\n\u001b[0;32m     21\u001b[0m     autoformer,\n\u001b[0;32m     22\u001b[0m     bark,\n\u001b[0;32m     23\u001b[0m     bart,\n\u001b[0;32m     24\u001b[0m     barthez,\n\u001b[0;32m     25\u001b[0m     bartpho,\n\u001b[0;32m     26\u001b[0m     beit,\n\u001b[0;32m     27\u001b[0m     bert,\n\u001b[0;32m     28\u001b[0m     bert_generation,\n\u001b[0;32m     29\u001b[0m     bert_japanese,\n\u001b[0;32m     30\u001b[0m     bertweet,\n\u001b[0;32m     31\u001b[0m     big_bird,\n\u001b[0;32m     32\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     33\u001b[0m     biogpt,\n\u001b[0;32m     34\u001b[0m     bit,\n\u001b[0;32m     35\u001b[0m     blenderbot,\n\u001b[0;32m     36\u001b[0m     blenderbot_small,\n\u001b[0;32m     37\u001b[0m     blip,\n\u001b[0;32m     38\u001b[0m     blip_2,\n\u001b[0;32m     39\u001b[0m     bloom,\n\u001b[0;32m     40\u001b[0m     bridgetower,\n\u001b[0;32m     41\u001b[0m     bros,\n\u001b[0;32m     42\u001b[0m     byt5,\n\u001b[0;32m     43\u001b[0m     camembert,\n\u001b[0;32m     44\u001b[0m     canine,\n\u001b[0;32m     45\u001b[0m     chinese_clip,\n\u001b[0;32m     46\u001b[0m     clap,\n\u001b[0;32m     47\u001b[0m     clip,\n\u001b[0;32m     48\u001b[0m     clipseg,\n\u001b[0;32m     49\u001b[0m     clvp,\n\u001b[0;32m     50\u001b[0m     code_llama,\n\u001b[0;32m     51\u001b[0m     codegen,\n\u001b[0;32m     52\u001b[0m     cohere,\n\u001b[0;32m     53\u001b[0m     conditional_detr,\n\u001b[0;32m     54\u001b[0m     convbert,\n\u001b[0;32m     55\u001b[0m     convnext,\n\u001b[0;32m     56\u001b[0m     convnextv2,\n\u001b[0;32m     57\u001b[0m     cpm,\n\u001b[0;32m     58\u001b[0m     cpmant,\n\u001b[0;32m     59\u001b[0m     ctrl,\n\u001b[0;32m     60\u001b[0m     cvt,\n\u001b[0;32m     61\u001b[0m     data2vec,\n\u001b[0;32m     62\u001b[0m     dbrx,\n\u001b[0;32m     63\u001b[0m     deberta,\n\u001b[0;32m     64\u001b[0m     deberta_v2,\n\u001b[0;32m     65\u001b[0m     decision_transformer,\n\u001b[0;32m     66\u001b[0m     deformable_detr,\n\u001b[0;32m     67\u001b[0m     deit,\n\u001b[0;32m     68\u001b[0m     deprecated,\n\u001b[0;32m     69\u001b[0m     depth_anything,\n\u001b[0;32m     70\u001b[0m     deta,\n\u001b[0;32m     71\u001b[0m     detr,\n\u001b[0;32m     72\u001b[0m     dialogpt,\n\u001b[0;32m     73\u001b[0m     dinat,\n\u001b[0;32m     74\u001b[0m     dinov2,\n\u001b[0;32m     75\u001b[0m     distilbert,\n\u001b[0;32m     76\u001b[0m     dit,\n\u001b[0;32m     77\u001b[0m     donut,\n\u001b[0;32m     78\u001b[0m     dpr,\n\u001b[0;32m     79\u001b[0m     dpt,\n\u001b[0;32m     80\u001b[0m     efficientformer,\n\u001b[0;32m     81\u001b[0m     efficientnet,\n\u001b[0;32m     82\u001b[0m     electra,\n\u001b[0;32m     83\u001b[0m     encodec,\n\u001b[0;32m     84\u001b[0m     encoder_decoder,\n\u001b[0;32m     85\u001b[0m     ernie,\n\u001b[0;32m     86\u001b[0m     ernie_m,\n\u001b[0;32m     87\u001b[0m     esm,\n\u001b[0;32m     88\u001b[0m     falcon,\n\u001b[0;32m     89\u001b[0m     fastspeech2_conformer,\n\u001b[0;32m     90\u001b[0m     flaubert,\n\u001b[0;32m     91\u001b[0m     flava,\n\u001b[0;32m     92\u001b[0m     fnet,\n\u001b[0;32m     93\u001b[0m     focalnet,\n\u001b[0;32m     94\u001b[0m     fsmt,\n\u001b[0;32m     95\u001b[0m     funnel,\n\u001b[0;32m     96\u001b[0m     fuyu,\n\u001b[0;32m     97\u001b[0m     gemma,\n\u001b[0;32m     98\u001b[0m     git,\n\u001b[0;32m     99\u001b[0m     glpn,\n\u001b[0;32m    100\u001b[0m     gpt2,\n\u001b[0;32m    101\u001b[0m     gpt_bigcode,\n\u001b[0;32m    102\u001b[0m     gpt_neo,\n\u001b[0;32m    103\u001b[0m     gpt_neox,\n\u001b[0;32m    104\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m    105\u001b[0m     gpt_sw3,\n\u001b[0;32m    106\u001b[0m     gptj,\n\u001b[0;32m    107\u001b[0m     gptsan_japanese,\n\u001b[0;32m    108\u001b[0m     graphormer,\n\u001b[0;32m    109\u001b[0m     grounding_dino,\n\u001b[0;32m    110\u001b[0m     groupvit,\n\u001b[0;32m    111\u001b[0m     herbert,\n\u001b[0;32m    112\u001b[0m     hubert,\n\u001b[0;32m    113\u001b[0m     ibert,\n\u001b[0;32m    114\u001b[0m     idefics,\n\u001b[0;32m    115\u001b[0m     idefics2,\n\u001b[0;32m    116\u001b[0m     imagegpt,\n\u001b[0;32m    117\u001b[0m     informer,\n\u001b[0;32m    118\u001b[0m     instructblip,\n\u001b[0;32m    119\u001b[0m     jamba,\n\u001b[0;32m    120\u001b[0m     jetmoe,\n\u001b[0;32m    121\u001b[0m     jukebox,\n\u001b[0;32m    122\u001b[0m     kosmos2,\n\u001b[0;32m    123\u001b[0m     layoutlm,\n\u001b[0;32m    124\u001b[0m     layoutlmv2,\n\u001b[0;32m    125\u001b[0m     layoutlmv3,\n\u001b[0;32m    126\u001b[0m     layoutxlm,\n\u001b[0;32m    127\u001b[0m     led,\n\u001b[0;32m    128\u001b[0m     levit,\n\u001b[0;32m    129\u001b[0m     lilt,\n\u001b[0;32m    130\u001b[0m     llama,\n\u001b[0;32m    131\u001b[0m     llava,\n\u001b[0;32m    132\u001b[0m     llava_next,\n\u001b[0;32m    133\u001b[0m     longformer,\n\u001b[0;32m    134\u001b[0m     longt5,\n\u001b[0;32m    135\u001b[0m     luke,\n\u001b[0;32m    136\u001b[0m     lxmert,\n\u001b[0;32m    137\u001b[0m     m2m_100,\n\u001b[0;32m    138\u001b[0m     mamba,\n\u001b[0;32m    139\u001b[0m     marian,\n\u001b[0;32m    140\u001b[0m     markuplm,\n\u001b[0;32m    141\u001b[0m     mask2former,\n\u001b[0;32m    142\u001b[0m     maskformer,\n\u001b[0;32m    143\u001b[0m     mbart,\n\u001b[0;32m    144\u001b[0m     mbart50,\n\u001b[0;32m    145\u001b[0m     mega,\n\u001b[0;32m    146\u001b[0m     megatron_bert,\n\u001b[0;32m    147\u001b[0m     megatron_gpt2,\n\u001b[0;32m    148\u001b[0m     mgp_str,\n\u001b[0;32m    149\u001b[0m     mistral,\n\u001b[0;32m    150\u001b[0m     mixtral,\n\u001b[0;32m    151\u001b[0m     mluke,\n\u001b[0;32m    152\u001b[0m     mobilebert,\n\u001b[0;32m    153\u001b[0m     mobilenet_v1,\n\u001b[0;32m    154\u001b[0m     mobilenet_v2,\n\u001b[0;32m    155\u001b[0m     mobilevit,\n\u001b[0;32m    156\u001b[0m     mobilevitv2,\n\u001b[0;32m    157\u001b[0m     mpnet,\n\u001b[0;32m    158\u001b[0m     mpt,\n\u001b[0;32m    159\u001b[0m     mra,\n\u001b[0;32m    160\u001b[0m     mt5,\n\u001b[0;32m    161\u001b[0m     musicgen,\n\u001b[0;32m    162\u001b[0m     musicgen_melody,\n\u001b[0;32m    163\u001b[0m     mvp,\n\u001b[0;32m    164\u001b[0m     nat,\n\u001b[0;32m    165\u001b[0m     nezha,\n\u001b[0;32m    166\u001b[0m     nllb,\n\u001b[0;32m    167\u001b[0m     nllb_moe,\n\u001b[0;32m    168\u001b[0m     nougat,\n\u001b[0;32m    169\u001b[0m     nystromformer,\n\u001b[0;32m    170\u001b[0m     olmo,\n\u001b[0;32m    171\u001b[0m     oneformer,\n\u001b[0;32m    172\u001b[0m     openai,\n\u001b[0;32m    173\u001b[0m     opt,\n\u001b[0;32m    174\u001b[0m     owlv2,\n\u001b[0;32m    175\u001b[0m     owlvit,\n\u001b[0;32m    176\u001b[0m     paligemma,\n\u001b[0;32m    177\u001b[0m     patchtsmixer,\n\u001b[0;32m    178\u001b[0m     patchtst,\n\u001b[0;32m    179\u001b[0m     pegasus,\n\u001b[0;32m    180\u001b[0m     pegasus_x,\n\u001b[0;32m    181\u001b[0m     perceiver,\n\u001b[0;32m    182\u001b[0m     persimmon,\n\u001b[0;32m    183\u001b[0m     phi,\n\u001b[0;32m    184\u001b[0m     phi3,\n\u001b[0;32m    185\u001b[0m     phobert,\n\u001b[0;32m    186\u001b[0m     pix2struct,\n\u001b[0;32m    187\u001b[0m     plbart,\n\u001b[0;32m    188\u001b[0m     poolformer,\n\u001b[0;32m    189\u001b[0m     pop2piano,\n\u001b[0;32m    190\u001b[0m     prophetnet,\n\u001b[0;32m    191\u001b[0m     pvt,\n\u001b[0;32m    192\u001b[0m     pvt_v2,\n\u001b[0;32m    193\u001b[0m     qdqbert,\n\u001b[0;32m    194\u001b[0m     qwen2,\n\u001b[0;32m    195\u001b[0m     qwen2_moe,\n\u001b[0;32m    196\u001b[0m     rag,\n\u001b[0;32m    197\u001b[0m     realm,\n\u001b[0;32m    198\u001b[0m     recurrent_gemma,\n\u001b[0;32m    199\u001b[0m     reformer,\n\u001b[0;32m    200\u001b[0m     regnet,\n\u001b[0;32m    201\u001b[0m     rembert,\n\u001b[0;32m    202\u001b[0m     resnet,\n\u001b[0;32m    203\u001b[0m     roberta,\n\u001b[0;32m    204\u001b[0m     roberta_prelayernorm,\n\u001b[0;32m    205\u001b[0m     roc_bert,\n\u001b[0;32m    206\u001b[0m     roformer,\n\u001b[0;32m    207\u001b[0m     rwkv,\n\u001b[0;32m    208\u001b[0m     sam,\n\u001b[0;32m    209\u001b[0m     seamless_m4t,\n\u001b[0;32m    210\u001b[0m     seamless_m4t_v2,\n\u001b[0;32m    211\u001b[0m     segformer,\n\u001b[0;32m    212\u001b[0m     seggpt,\n\u001b[0;32m    213\u001b[0m     sew,\n\u001b[0;32m    214\u001b[0m     sew_d,\n\u001b[0;32m    215\u001b[0m     siglip,\n\u001b[0;32m    216\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    217\u001b[0m     speech_to_text,\n\u001b[0;32m    218\u001b[0m     speech_to_text_2,\n\u001b[0;32m    219\u001b[0m     speecht5,\n\u001b[0;32m    220\u001b[0m     splinter,\n\u001b[0;32m    221\u001b[0m     squeezebert,\n\u001b[0;32m    222\u001b[0m     stablelm,\n\u001b[0;32m    223\u001b[0m     starcoder2,\n\u001b[0;32m    224\u001b[0m     superpoint,\n\u001b[0;32m    225\u001b[0m     swiftformer,\n\u001b[0;32m    226\u001b[0m     swin,\n\u001b[0;32m    227\u001b[0m     swin2sr,\n\u001b[0;32m    228\u001b[0m     swinv2,\n\u001b[0;32m    229\u001b[0m     switch_transformers,\n\u001b[0;32m    230\u001b[0m     t5,\n\u001b[0;32m    231\u001b[0m     table_transformer,\n\u001b[0;32m    232\u001b[0m     tapas,\n\u001b[0;32m    233\u001b[0m     time_series_transformer,\n\u001b[0;32m    234\u001b[0m     timesformer,\n\u001b[0;32m    235\u001b[0m     timm_backbone,\n\u001b[0;32m    236\u001b[0m     trocr,\n\u001b[0;32m    237\u001b[0m     tvlt,\n\u001b[0;32m    238\u001b[0m     tvp,\n\u001b[0;32m    239\u001b[0m     udop,\n\u001b[0;32m    240\u001b[0m     umt5,\n\u001b[0;32m    241\u001b[0m     unispeech,\n\u001b[0;32m    242\u001b[0m     unispeech_sat,\n\u001b[0;32m    243\u001b[0m     univnet,\n\u001b[0;32m    244\u001b[0m     upernet,\n\u001b[0;32m    245\u001b[0m     video_llava,\n\u001b[0;32m    246\u001b[0m     videomae,\n\u001b[0;32m    247\u001b[0m     vilt,\n\u001b[0;32m    248\u001b[0m     vipllava,\n\u001b[0;32m    249\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    250\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    251\u001b[0m     visual_bert,\n\u001b[0;32m    252\u001b[0m     vit,\n\u001b[0;32m    253\u001b[0m     vit_hybrid,\n\u001b[0;32m    254\u001b[0m     vit_mae,\n\u001b[0;32m    255\u001b[0m     vit_msn,\n\u001b[0;32m    256\u001b[0m     vitdet,\n\u001b[0;32m    257\u001b[0m     vitmatte,\n\u001b[0;32m    258\u001b[0m     vits,\n\u001b[0;32m    259\u001b[0m     vivit,\n\u001b[0;32m    260\u001b[0m     wav2vec2,\n\u001b[0;32m    261\u001b[0m     wav2vec2_bert,\n\u001b[0;32m    262\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    263\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    264\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    265\u001b[0m     wavlm,\n\u001b[0;32m    266\u001b[0m     whisper,\n\u001b[0;32m    267\u001b[0m     x_clip,\n\u001b[0;32m    268\u001b[0m     xglm,\n\u001b[0;32m    269\u001b[0m     xlm,\n\u001b[0;32m    270\u001b[0m     xlm_prophetnet,\n\u001b[0;32m    271\u001b[0m     xlm_roberta,\n\u001b[0;32m    272\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    273\u001b[0m     xlnet,\n\u001b[0;32m    274\u001b[0m     xmod,\n\u001b[0;32m    275\u001b[0m     yolos,\n\u001b[0;32m    276\u001b[0m     yoso,\n\u001b[0;32m    277\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\models\\mt5\\__init__.py:36\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_t5_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5TokenizerFast\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sentencepiece_available, logging\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_tokenizers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpre_tokenizers_fast\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoding \u001b[38;5;28;01mas\u001b[39;00m EncodingFast\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\tokenizers\\__init__.py:78\u001b[0m\n\u001b[0;32m     75\u001b[0m     CONTIGUOUS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontiguous\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     79\u001b[0m     AddedToken,\n\u001b[0;32m     80\u001b[0m     Encoding,\n\u001b[0;32m     81\u001b[0m     NormalizedString,\n\u001b[0;32m     82\u001b[0m     PreTokenizedString,\n\u001b[0;32m     83\u001b[0m     Regex,\n\u001b[0;32m     84\u001b[0m     Token,\n\u001b[0;32m     85\u001b[0m     Tokenizer,\n\u001b[0;32m     86\u001b[0m     decoders,\n\u001b[0;32m     87\u001b[0m     models,\n\u001b[0;32m     88\u001b[0m     normalizers,\n\u001b[0;32m     89\u001b[0m     pre_tokenizers,\n\u001b[0;32m     90\u001b[0m     processors,\n\u001b[0;32m     91\u001b[0m     trainers,\n\u001b[0;32m     92\u001b[0m     __version__,\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     95\u001b[0m     BertWordPieceTokenizer,\n\u001b[0;32m     96\u001b[0m     ByteLevelBPETokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     SentencePieceUnigramTokenizer,\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tokenizers.tokenizers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mddsp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F0_Extractor, Volume_Extractor, Units_Encoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocoder\n",
      "File \u001b[1;32mC:\\ai_voice\\DDSP-SVC-KOR-master\\preprocess.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mddsp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F0_Extractor, Volume_Extractor, Units_Encoder\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocoder\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traverse_dir\n",
      "File \u001b[1;32mC:\\ai_voice\\DDSP-SVC-KOR-master\\ddsp\\vocoder.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchcrepe\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mresampy\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubertModel, Wav2Vec2FeatureExtractor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_utils\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mencoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhubert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubertSoft\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1525\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1525\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch201_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1537\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1540\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.hubert because of the following error (look up to see its traceback):\nNo module named 'tokenizers.tokenizers'"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocess\n",
    "from ddsp.vocoder import F0_Extractor, Volume_Extractor, Units_Encoder\n",
    "from diffusion.vocoder import Vocoder\n",
    "\n",
    "# get data\n",
    "sample_rate = args.data.sampling_rate\n",
    "hop_size = args.data.block_size\n",
    "\n",
    "# initialize f0 extractor\n",
    "f0_extractor = F0_Extractor(\n",
    "                    args.data.f0_extractor, \n",
    "                    args.data.sampling_rate, \n",
    "                    args.data.block_size, \n",
    "                    args.data.f0_min, \n",
    "                    args.data.f0_max)\n",
    "\n",
    "# initialize volume extractor\n",
    "volume_extractor = Volume_Extractor(args.data.block_size)\n",
    "\n",
    "# initialize mel extractor\n",
    "mel_extractor = None\n",
    "if args.model.type == 'Diffusion':\n",
    "    mel_extractor = Vocoder(args.vocoder.type, args.vocoder.ckpt, device = device)\n",
    "    if mel_extractor.vocoder_sample_rate != sample_rate or mel_extractor.vocoder_hop_size != hop_size:\n",
    "        mel_extractor = None\n",
    "        print('Unmatch vocoder parameters, mel extraction is ignored!')\n",
    "\n",
    "# initialize units encoder\n",
    "if args.data.encoder == 'cnhubertsoftfish':\n",
    "    cnhubertsoft_gate = args.data.cnhubertsoft_gate\n",
    "else:\n",
    "    cnhubertsoft_gate = 10             \n",
    "units_encoder = Units_Encoder(\n",
    "                    args.data.encoder, \n",
    "                    args.data.encoder_ckpt, \n",
    "                    args.data.encoder_sample_rate, \n",
    "                    args.data.encoder_hop_size, \n",
    "                    device = device)    \n",
    "\n",
    "# preprocess training set\n",
    "preprocess(args.data.train_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device)\n",
    "\n",
    "# preprocess validation set\n",
    "preprocess(args.data.valid_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >    exp: exp/sins-test\n",
      " [DDSP Model] Sinusoids Additive Synthesiser\n",
      "Load all the data from : data/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:01<00:00, 58.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load all the data from : data/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 57.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- model size ---\n",
      "model: 3,375,360\n",
      "======= start training =======\n",
      "epoch: 1 |   4/  5 | exp/sins-test | batch/s: 2.16 | loss: 5.180 | time: 0:00:04.6 | step: 10\n",
      "epoch: 3 |   4/  5 | exp/sins-test | batch/s: 7.92 | loss: 4.407 | time: 0:00:05.9 | step: 20\n",
      "epoch: 5 |   4/  5 | exp/sins-test | batch/s: 7.92 | loss: 3.218 | time: 0:00:07.1 | step: 30\n",
      "epoch: 7 |   4/  5 | exp/sins-test | batch/s: 7.85 | loss: 2.494 | time: 0:00:08.4 | step: 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m ddsp_train\n\u001b[1;32m----> 3\u001b[0m ddsp_train(args)\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\project\\DDSP-SVC-KOR\\train.py:156\u001b[0m, in \u001b[0;36mddsp_train\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    153\u001b[0m loader_train, loader_valid \u001b[39m=\u001b[39m get_data_loaders(args, whole_audio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    155\u001b[0m \u001b[39m# run\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m train(args, initial_global_step, model, optimizer, loss_func, loader_train, loader_valid)\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\project\\DDSP-SVC-KOR\\solver.py:84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, initial_global_step, model, optimizer, loss_func, loader_train, loader_test)\u001b[0m\n\u001b[0;32m     82\u001b[0m saver\u001b[39m.\u001b[39mlog_info(\u001b[39m'\u001b[39m\u001b[39m======= start training =======\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader_train):\n\u001b[0;32m     85\u001b[0m         saver\u001b[39m.\u001b[39mglobal_step_increment()\n\u001b[0;32m     86\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\wlsdm\\anaconda3\\envs\\ddsp\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import ddsp_train\n",
    "\n",
    "ddsp_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과물 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [DDSP Model] Combtooth Subtractive Synthesiser\n",
      " [Loading] exp/combsub-test/model_best.pt\n",
      "MD5: 44aa2cc3a0a8aa04bde53b9e9b7e729c\n",
      "Pitch extractor type: crepe\n",
      "Extracting the pitch curve of the input audio...\n",
      "Extracting the volume envelope of the input audio...\n",
      " [Encoder Model] HuBERT Soft\n",
      " [Loading] pretrain/hubert/hubert-soft-0d54a1f4.pt\n",
      "Enhancer type: nsf-hifigan\n",
      "| Load HifiGAN:  pretrain/nsf_hifigan/model\n",
      "Removing weight norm...\n",
      "Speaker ID: 1\n",
      "Cut the input audio into 3 slices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from main import inference\n",
    "# configure setting\n",
    "configures = {\n",
    "    'model_path'            :   'exp/combsub-test/model_best.pt', # 추론에 사용하고자 하는 모델, 바로위에서 학습한 모델을 가져오면댐\n",
    "    'input'                 :   'data/train/audio/video-0000.wav', # 추론하고자 하는 노래파일의 위치 - 님들이 바꿔야댐 \n",
    "    'output'                :   'output.wav',  # 결과물 파일의 위치\n",
    "    'device'                :   'cuda',\n",
    "    'spk_id'                :   '1', \n",
    "    'spk_mix_dict'          :   'None', \n",
    "    'key'                   :   '0', \n",
    "    'enhance'               :   'true' , \n",
    "    'pitch_extractor'       :   'crepe' ,\n",
    "    'f0_min'                :   '50' ,\n",
    "    'f0_max'                :   '1100',\n",
    "    'threhold'              :   '-60',\n",
    "    'enhancer_adaptive_key' :   '0'\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "inference(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8a643f5fe528358e1cfac3836870fd104c9c787e6f994a831162d9d1f5f0281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
